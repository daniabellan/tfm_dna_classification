# =============================
# Training Configuration
# =============================

# Number of samples per training step (batch size).
batch_size: 16

# Initial learning rate for the optimizer.
learning_rate: 0.0001

# Number of complete passes through the dataset during training.
epochs: 200

# Random seed for experiment reproducibility.
seed: 42

# =============================
# Optimizer Settings
# =============================
optimizer:
  # Type of optimizer used for training (Adam in this case).
  type: Adam

  # L2 weight decay factor for regularization (helps prevent overfitting).
  weight_decay: 0.0001

# =============================
# Learning Rate Scheduler
# =============================
scheduler:
  # Number of epochs after which the learning rate is reduced.
  step_size: 10

  # Factor by which the learning rate is reduced every `step_size` epochs.
  gamma: 0.1

# =============================
# Early Stopping
# =============================
early_stopping:
  # Number of epochs without improvement in validation metrics before stopping training.
  patience: 5

  # Minimum improvement in validation metric required to reset patience.
  min_delta: 0.0001

# =============================
# Gradient Clipping
# =============================
gradient_clipping:
  # Maximum gradient norm to prevent gradient explosion.
  max_grad_norm: 1.0
